{
    "experience": "[\n  {\n    \"question\": \"During your internship at Avinyaz, you designed an entire microservice. Can you elaborate on the architectural decisions you made, specifically concerning the communication patterns between this new microservice and the existing ones? What considerations led you to choose a synchronous or asynchronous approach, and how did you handle potential failures or inconsistencies in a distributed environment?\",\n    \"area\": \"Microservice Architecture & Distributed Systems\"\n  },\n  {\n    \"question\": \"You mentioned integrating LangChain for OpenAI models at Avinyaz. Could you describe a specific challenge you encountered while integrating LangChain with your backend services? What strategies did you employ to optimize the performance of the LangChain integration, considering the potential latency associated with OpenAI API calls, and how did you manage API rate limits?\",\n    \"area\": \"AI Integration & Performance Optimization\"\n  },\n  {\n    \"question\": \"At Avinyaz, you independently designed DynamoDB tables. Can you walk me through the process of analyzing schema requirements and designing these tables? What specific DynamoDB features (e.g., Global Secondary Indexes, Local Secondary Indexes, DAX) did you utilize, and why? How did you ensure efficient query patterns and minimize read/write costs, especially given the eventual scale of the application?\",\n    \"area\": \"NoSQL Database Design & Optimization\"\n  },\n  {\n    \"question\": \"During your time as Technical Head at Oculus S.P.I.T., you developed a real-time IPL auction app. Considering the real-time nature of the application and the potential for concurrent updates from multiple users, how did you handle data consistency and prevent race conditions in your API endpoints? Can you describe the specific techniques you used to ensure data integrity within the MongoDB database?\",\n    \"area\": \"Real-Time Application Development & Concurrency Control\"\n  },\n  {\n    \"question\": \"At Avinyaz, you created an API & token usage monitoring system that resulted in a 30% cost reduction. Can you describe the data analysis techniques you employed to identify areas for optimization? What specific metrics did you track, and how did you use this information to optimize API usage and reduce costs associated with AWS services (e.g., DynamoDB read/write capacity, API Gateway usage)? What tools were used for data collection and analysis?\",\n    \"area\": \"Data Analysis, Monitoring & Cost Optimization\"\n  }\n]"
}